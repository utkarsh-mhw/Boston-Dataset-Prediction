{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((379, 14), (379, 27), (379,), (127, 27))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.genfromtxt('Downloads/0000000000002417_training_boston_x_y_train.csv', delimiter=',')\n",
    "X_train = data[:, 0:13]\n",
    "Y_train = data[:, 13]\n",
    "\n",
    "columns = X_train.shape[1]\n",
    "\n",
    "# self pair\n",
    "for i in range(columns):\n",
    "    new_column = (X_train[:, i] * X_train[:, i]).reshape(-1,1)\n",
    "    X_train = np.append(X_train, new_column, axis=1)\n",
    "    \n",
    "\n",
    "ones = np.ones(X_train.shape[0]).reshape(-1, 1)\n",
    "X_train = np.append(X_train, ones, axis=1)\n",
    "\n",
    "X_test = np.genfromtxt('Downloads/0000000000002417_test_boston_x_test.csv', delimiter=',')\n",
    "\n",
    "\n",
    "columns = X_test.shape[1]\n",
    "\n",
    "# self pair\n",
    "for i in range(columns):\n",
    "    new_column = (X_test[:, i] * X_test[:, i]).reshape(-1,1)\n",
    "    X_test = np.append(X_test, new_column, axis=1)\n",
    "\n",
    "test_ones = np.ones(X_test.shape[0]).reshape(-1, 1)\n",
    "X_test = np.append(X_test, test_ones, axis=1)\n",
    "\n",
    "\n",
    "data.shape, X_train.shape, Y_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.407850</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-1.266023</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.576134</td>\n",
       "      <td>1.239974</td>\n",
       "      <td>0.840122</td>\n",
       "      <td>-0.520264</td>\n",
       "      <td>-0.752922</td>\n",
       "      <td>-1.278354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.331930</td>\n",
       "      <td>1.537535</td>\n",
       "      <td>0.705805</td>\n",
       "      <td>0.270675</td>\n",
       "      <td>0.566892</td>\n",
       "      <td>1.634190</td>\n",
       "      <td>0.091866</td>\n",
       "      <td>0.168569</td>\n",
       "      <td>1.205582</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.407374</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>0.247057</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-1.016689</td>\n",
       "      <td>0.001946</td>\n",
       "      <td>-0.838337</td>\n",
       "      <td>0.336351</td>\n",
       "      <td>-0.523001</td>\n",
       "      <td>-0.060801</td>\n",
       "      <td>...</td>\n",
       "      <td>1.033656</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.702809</td>\n",
       "      <td>0.113132</td>\n",
       "      <td>0.273531</td>\n",
       "      <td>0.003697</td>\n",
       "      <td>0.012776</td>\n",
       "      <td>0.084779</td>\n",
       "      <td>0.270893</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.125179</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>1.015999</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>1.367490</td>\n",
       "      <td>-0.439699</td>\n",
       "      <td>0.687212</td>\n",
       "      <td>-0.577309</td>\n",
       "      <td>1.661245</td>\n",
       "      <td>1.530926</td>\n",
       "      <td>...</td>\n",
       "      <td>1.870030</td>\n",
       "      <td>0.193335</td>\n",
       "      <td>0.472260</td>\n",
       "      <td>0.333285</td>\n",
       "      <td>2.759736</td>\n",
       "      <td>2.343736</td>\n",
       "      <td>0.650565</td>\n",
       "      <td>14.408063</td>\n",
       "      <td>0.794016</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.028304</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>1.015999</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>1.859875</td>\n",
       "      <td>-0.047918</td>\n",
       "      <td>0.801005</td>\n",
       "      <td>-0.712836</td>\n",
       "      <td>1.661245</td>\n",
       "      <td>1.530926</td>\n",
       "      <td>...</td>\n",
       "      <td>3.459136</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.641610</td>\n",
       "      <td>0.508136</td>\n",
       "      <td>2.759736</td>\n",
       "      <td>2.343736</td>\n",
       "      <td>0.650565</td>\n",
       "      <td>0.004363</td>\n",
       "      <td>0.046414</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.412408</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-0.969827</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.913029</td>\n",
       "      <td>-0.384137</td>\n",
       "      <td>-0.834781</td>\n",
       "      <td>0.300508</td>\n",
       "      <td>-0.752922</td>\n",
       "      <td>-0.957633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833622</td>\n",
       "      <td>0.147561</td>\n",
       "      <td>0.696859</td>\n",
       "      <td>0.090305</td>\n",
       "      <td>0.566892</td>\n",
       "      <td>0.917061</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.185825</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.407850 -0.487722 -1.266023 -0.272599 -0.576134  1.239974  0.840122   \n",
       "1 -0.407374 -0.487722  0.247057 -0.272599 -1.016689  0.001946 -0.838337   \n",
       "2  0.125179 -0.487722  1.015999 -0.272599  1.367490 -0.439699  0.687212   \n",
       "3  0.028304 -0.487722  1.015999 -0.272599  1.859875 -0.047918  0.801005   \n",
       "4 -0.412408 -0.487722 -0.969827 -0.272599 -0.913029 -0.384137 -0.834781   \n",
       "\n",
       "         7         8         9   ...        17        18        19        20  \\\n",
       "0 -0.520264 -0.752922 -1.278354  ...  0.331930  1.537535  0.705805  0.270675   \n",
       "1  0.336351 -0.523001 -0.060801  ...  1.033656  0.000004  0.702809  0.113132   \n",
       "2 -0.577309  1.661245  1.530926  ...  1.870030  0.193335  0.472260  0.333285   \n",
       "3 -0.712836  1.661245  1.530926  ...  3.459136  0.002296  0.641610  0.508136   \n",
       "4  0.300508 -0.752922 -0.957633  ...  0.833622  0.147561  0.696859  0.090305   \n",
       "\n",
       "         21        22        23         24        25   26  \n",
       "0  0.566892  1.634190  0.091866   0.168569  1.205582  1.0  \n",
       "1  0.273531  0.003697  0.012776   0.084779  0.270893  1.0  \n",
       "2  2.759736  2.343736  0.650565  14.408063  0.794016  1.0  \n",
       "3  2.759736  2.343736  0.650565   0.004363  0.046414  1.0  \n",
       "4  0.566892  0.917061  0.000423   0.185825  0.000841  1.0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(X_train)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient(X_train, Y_train, learning_rate, m,j):\n",
    "    m_j = 0\n",
    "    n_data_pts = X_train.shape[0]\n",
    "    N = len(m)\n",
    "    for i in range(n_data_pts):\n",
    "        x_i = X_train[i, :]\n",
    "        y_i = Y_train[i]\n",
    "        temp_sum = 0\n",
    "        for k in range(N):\n",
    "            temp_sum += m[k]*x_i[k]\n",
    "        temp_sum = y_i - temp_sum\n",
    "        ## complete formula\n",
    "        m_j += (-2/n_data_pts) * (temp_sum) * x_i[j]\n",
    "    # update m[j] and return\n",
    "    m[j] = m[j] - (learning_rate*m_j)\n",
    "    return m[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gradient_descent(X_train, Y_train, learning_rate, num_iterations):\n",
    "    m = [0]*(X_train.shape[1])\n",
    "    m[-1] = 1 #c\n",
    "    N = len(m)\n",
    "    for i in range(num_iterations):\n",
    "        for j in range(N):\n",
    "            m[j] = step_gradient(X_train, Y_train, learning_rate, m,j)\n",
    "        a = cost(X_train, Y_train, m)\n",
    "        if (i%10==0) or (i ==(num_iterations-1)):\n",
    "            print(\"iteration\",i,\" cost:\",a)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(X_train, Y_train, m):\n",
    "    cost = 0\n",
    "    n_data_pts = len(X_train)\n",
    "    N = len(m)\n",
    "    for i in range(n_data_pts):\n",
    "        x_i = X_train[i, :]\n",
    "        y_i = Y_train[i]\n",
    "        temp_sum = 0\n",
    "        for k in range(N):\n",
    "            temp_sum += m[k]*x_i[k]\n",
    "        temp_sum = y_i - temp_sum\n",
    "        cost += (1/n_data_pts) * ((temp_sum)**2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, m):\n",
    "    n_fts = X_train.shape[1]\n",
    "    n_m = np.array(m).reshape(n_fts, 1)\n",
    "    return np.dot(X_test, n_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    num_iterations = 1200\n",
    "    learning_rate = 0.01\n",
    "    m = gradient_descent(X_train, Y_train, learning_rate, num_iterations)\n",
    "    print(m)\n",
    "    Y_pred = predict(X_test, m)\n",
    "    np.savetxt('boston_prediction_3.csv', Y_pred, fmt='%.5f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0  cost: 354.20571397528494\n",
      "iteration 10  cost: 102.43595997833758\n",
      "iteration 20  cost: 68.81579278204018\n",
      "iteration 30  cost: 53.53585736281988\n",
      "iteration 40  cost: 45.430765428085714\n",
      "iteration 50  cost: 40.6389704281519\n",
      "iteration 60  cost: 37.50665809640333\n",
      "iteration 70  cost: 35.26756608924322\n",
      "iteration 80  cost: 33.548249484896324\n",
      "iteration 90  cost: 32.15721786084117\n",
      "iteration 100  cost: 30.990334890990855\n",
      "iteration 110  cost: 29.987042865551086\n",
      "iteration 120  cost: 29.109536783225376\n",
      "iteration 130  cost: 28.332549838864463\n",
      "iteration 140  cost: 27.63815832325309\n",
      "iteration 150  cost: 27.013021231219938\n",
      "iteration 160  cost: 26.446836380751037\n",
      "iteration 170  cost: 25.931426507732517\n",
      "iteration 180  cost: 25.46016585797129\n",
      "iteration 190  cost: 25.02760012194627\n",
      "iteration 200  cost: 24.629182200343504\n",
      "iteration 210  cost: 24.26108121938424\n",
      "iteration 220  cost: 23.920040260799485\n",
      "iteration 230  cost: 23.603267929146913\n",
      "iteration 240  cost: 23.30835426587369\n",
      "iteration 250  cost: 23.033204666326288\n",
      "iteration 260  cost: 22.77598738615079\n",
      "iteration 270  cost: 22.535091465646705\n",
      "iteration 280  cost: 22.30909273562958\n",
      "iteration 290  cost: 22.096726150919128\n",
      "iteration 300  cost: 21.89686311633856\n",
      "iteration 310  cost: 21.70849277814016\n",
      "iteration 320  cost: 21.530706484288704\n",
      "iteration 330  cost: 21.36268479175804\n",
      "iteration 340  cost: 21.203686532703124\n",
      "iteration 350  cost: 21.05303955442403\n",
      "iteration 360  cost: 20.91013282792791\n",
      "iteration 370  cost: 20.774409682110942\n",
      "iteration 380  cost: 20.645361969235548\n",
      "iteration 390  cost: 20.522525005567427\n",
      "iteration 400  cost: 20.405473161116408\n",
      "iteration 410  cost: 20.293815996205236\n",
      "iteration 420  cost: 20.18719486145448\n",
      "iteration 430  cost: 20.08527989279471\n",
      "iteration 440  cost: 19.987767345126457\n",
      "iteration 450  cost: 19.894377217888138\n",
      "iteration 460  cost: 19.804851133564505\n",
      "iteration 470  cost: 19.718950436461704\n",
      "iteration 480  cost: 19.636454484195998\n",
      "iteration 490  cost: 19.55715910853121\n",
      "iteration 500  cost: 19.480875225641146\n",
      "iteration 510  cost: 19.40742757871685\n",
      "iteration 520  cost: 19.336653598199533\n",
      "iteration 530  cost: 19.26840236689287\n",
      "iteration 540  cost: 19.20253367886327\n",
      "iteration 550  cost: 19.13891718243483\n",
      "iteration 560  cost: 19.077431598771213\n",
      "iteration 570  cost: 19.017964008548674\n",
      "iteration 580  cost: 18.96040920009211\n",
      "iteration 590  cost: 18.904669073092194\n",
      "iteration 600  cost: 18.85065209267017\n",
      "iteration 610  cost: 18.798272789117807\n",
      "iteration 620  cost: 18.747451299132205\n",
      "iteration 630  cost: 18.69811294479491\n",
      "iteration 640  cost: 18.65018784692438\n",
      "iteration 650  cost: 18.603610569764665\n",
      "iteration 660  cost: 18.558319794269718\n",
      "iteration 670  cost: 18.51425801750644\n",
      "iteration 680  cost: 18.471371275933397\n",
      "iteration 690  cost: 18.429608890522516\n",
      "iteration 700  cost: 18.388923231877644\n",
      "iteration 710  cost: 18.349269503673455\n",
      "iteration 720  cost: 18.310605542888137\n",
      "iteration 730  cost: 18.27289163544084\n",
      "iteration 740  cost: 18.236090345967007\n",
      "iteration 750  cost: 18.200166360576095\n",
      "iteration 760  cost: 18.16508634153666\n",
      "iteration 770  cost: 18.130818792925172\n",
      "iteration 780  cost: 18.097333936357014\n",
      "iteration 790  cost: 18.064603595993418\n",
      "iteration 800  cost: 18.032601092086477\n",
      "iteration 810  cost: 18.001301142385966\n",
      "iteration 820  cost: 17.970679770788628\n",
      "iteration 830  cost: 17.94071422266174\n",
      "iteration 840  cost: 17.911382886319977\n",
      "iteration 850  cost: 17.8826652201774\n",
      "iteration 860  cost: 17.854541685135118\n",
      "iteration 870  cost: 17.82699368180196\n",
      "iteration 880  cost: 17.800003492176923\n",
      "iteration 890  cost: 17.77355422545294\n",
      "iteration 900  cost: 17.74762976762887\n",
      "iteration 910  cost: 17.72221473464106\n",
      "iteration 920  cost: 17.69729442874959\n",
      "iteration 930  cost: 17.672854797934697\n",
      "iteration 940  cost: 17.64888239807887\n",
      "iteration 950  cost: 17.625364357726998\n",
      "iteration 960  cost: 17.602288345233998\n",
      "iteration 970  cost: 17.579642538123807\n",
      "iteration 980  cost: 17.557415594497403\n",
      "iteration 990  cost: 17.535596626340222\n",
      "iteration 1000  cost: 17.514175174590804\n",
      "iteration 1010  cost: 17.49314118584326\n",
      "iteration 1020  cost: 17.47248499056557\n",
      "iteration 1030  cost: 17.452197282725574\n",
      "iteration 1040  cost: 17.432269100723364\n",
      "iteration 1050  cost: 17.41269180953813\n",
      "iteration 1060  cost: 17.393457084003067\n",
      "iteration 1070  cost: 17.374556893129174\n",
      "iteration 1080  cost: 17.355983485404625\n",
      "iteration 1090  cost: 17.337729375001818\n",
      "iteration 1100  cost: 17.319787328828927\n",
      "iteration 1110  cost: 17.30215035436827\n",
      "iteration 1120  cost: 17.284811688247114\n",
      "iteration 1130  cost: 17.267764785491405\n",
      "iteration 1140  cost: 17.25100330941574\n",
      "iteration 1150  cost: 17.23452112210712\n",
      "iteration 1160  cost: 17.218312275462196\n",
      "iteration 1170  cost: 17.202371002741707\n",
      "iteration 1180  cost: 17.18669171060723\n",
      "iteration 1190  cost: 17.171268971609184\n",
      "iteration 1199  cost: 17.157603501729692\n",
      "[-3.6315745668403845, -1.8040494762223251, -0.5544660546564754, -3.3552940942758593, -3.6873393862753048, 1.4659634051245054, 0.6169968289647271, -3.601464286105129, 1.2153360042092363, -1.4316511650470034, -1.5198735539790926, 0.818436233741947, -5.697196473962096, 0.2748686308672474, 0.2707531868440924, 0.3003715041951, 1.2480747552789808, 0.20118199102567627, 0.8060216736096947, 0.5304356380545222, 0.9103786163830109, 1.9720727459677456, 0.4539494212051364, 0.8883645412340868, 0.10325153600328653, 1.31950589480136, 13.25675726079434]\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
